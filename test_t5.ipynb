{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T00:41:31.266091Z","iopub.status.busy":"2023-11-24T00:41:31.265706Z","iopub.status.idle":"2023-11-24T00:41:32.244147Z","shell.execute_reply":"2023-11-24T00:41:32.242942Z","shell.execute_reply.started":"2023-11-24T00:41:31.266065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Dec 15 14:25:56 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.216.04   Driver Version: 450.216.04   CUDA Version: 11.5     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-80GB      On   | 00000000:B7:00.0 Off |                    0 |\n","| N/A   31C    P0    61W / 400W |      0MiB / 81252MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T00:42:00.269822Z","iopub.status.busy":"2023-11-24T00:42:00.268979Z","iopub.status.idle":"2023-11-24T00:42:13.025629Z","shell.execute_reply":"2023-11-24T00:42:13.024791Z","shell.execute_reply.started":"2023-11-24T00:42:00.269791Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2023-12-15 22:54:41,807] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]}],"source":["import re\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import torch\n","from transformers import GPT2Tokenizer, T5ForConditionalGeneration, GenerationConfig\n","from peft import PeftModel, PeftConfig\n","from tqdm import tqdm\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T00:42:13.027334Z","iopub.status.busy":"2023-11-24T00:42:13.026936Z","iopub.status.idle":"2023-11-24T00:42:13.035114Z","shell.execute_reply":"2023-11-24T00:42:13.034188Z","shell.execute_reply.started":"2023-11-24T00:42:13.027299Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda:0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T00:42:13.036510Z","iopub.status.busy":"2023-11-24T00:42:13.036198Z","iopub.status.idle":"2023-11-24T00:55:53.741578Z","shell.execute_reply":"2023-11-24T00:55:53.740510Z","shell.execute_reply.started":"2023-11-24T00:42:13.036484Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["(device(type='cuda', index=0), './models/brackets/ep6')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# MODEL_CHKP = 'calculator/ep2'\n","MODEL_CHKP = 'brackets/ep6'\n","# MODEL_CHKP = 'ilya'\n","MODEL_NAME = f'./models/{MODEL_CHKP}'\n","BASE_MODEL = 'ai-forever/FRED-T5-1.7B'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(BASE_MODEL, eos_token='</s>')\n","model = T5ForConditionalGeneration.from_pretrained(\n","    BASE_MODEL,\n","    device_map=device,\n","    # torch_dtype=torch.float16,\n","    # load_in_8bit=False,\n",")\n","peft_config = PeftConfig.from_pretrained(MODEL_NAME)\n","model = PeftModel.from_pretrained(\n","    model,\n","    MODEL_NAME,\n","    device_map=device,\n","    # torch_dtype=torch.float16,\n","    # use_safetensors=True,\n",")\n","# model.half()\n","model.eval()\n","model.device, MODEL_NAME"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T00:59:39.946420Z","iopub.status.busy":"2023-11-24T00:59:39.946020Z","iopub.status.idle":"2023-11-24T00:59:39.953715Z","shell.execute_reply":"2023-11-24T00:59:39.952857Z","shell.execute_reply.started":"2023-11-24T00:59:39.946385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"greedy\": true,\n","  \"max_new_tokens\": 1000,\n","  \"no_repeat_ngram_size\": 15,\n","  \"pad_token_id\": 0,\n","  \"repetition_penalty\": 1.01\n","}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["generation_config = GenerationConfig(**{\n","    \"pad_token_id\": 0,\n","    \"bos_token_id\": 1,\n","    \"eos_token_id\": 2,\n","    \"max_new_tokens\": 1000,\n","    \"no_repeat_ngram_size\": 15,\n","    \"repetition_penalty\": 1.01,\n","\n","    \"greedy\": True,\n","    \"do_sample\": False,\n","\n","    # \"do_sample\": True,\n","    # \"temperature\": 0.1,\n","    # \"top_k\": 10,\n","    # \"top_p\": 0.95\n","})\n","generation_config"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T01:01:15.035806Z","iopub.status.busy":"2023-11-24T01:01:15.034775Z","iopub.status.idle":"2023-11-24T01:01:15.047714Z","shell.execute_reply":"2023-11-24T01:01:15.046722Z","shell.execute_reply.started":"2023-11-24T01:01:15.035772Z"}},"outputs":[],"source":["\"\"\"\n","Helpers to support streaming generate output.\n","Borrowed from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/callbacks.py\n","\"\"\"\n","\n","import gc\n","import traceback\n","from queue import Queue\n","from threading import Thread\n","from transformers import StoppingCriteria, StoppingCriteriaList\n","\n","class Stream(StoppingCriteria):\n","    def __init__(self, callback_func=None):\n","        self.callback_func = callback_func\n","\n","    def __call__(self, input_ids, scores) -> bool:\n","        if self.callback_func is not None:\n","            self.callback_func(input_ids[0])\n","        return False\n","\n","\n","class Iteratorize:\n","    \"\"\"\n","    Transforms a function that takes a callback\n","    into a lazy iterator (generator).\n","    \"\"\"\n","\n","    def __init__(self, func, kwargs={}, callback=None):\n","        self.mfunc = func\n","        self.c_callback = callback\n","        self.q = Queue()\n","        self.sentinel = object()\n","        self.kwargs = kwargs\n","        self.stop_now = False\n","\n","        def _callback(val):\n","            if self.stop_now:\n","                raise ValueError\n","            self.q.put(val)\n","\n","        def gentask():\n","            try:\n","                ret = self.mfunc(callback=_callback, **self.kwargs)\n","            except ValueError:\n","                pass\n","            except:\n","                traceback.print_exc()\n","                pass\n","\n","            self.q.put(self.sentinel)\n","            if self.c_callback:\n","                self.c_callback(ret)\n","\n","        self.thread = Thread(target=gentask)\n","        self.thread.start()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        obj = self.q.get(True, None)\n","        if obj is self.sentinel:\n","            raise StopIteration\n","        else:\n","            return obj\n","\n","    def __enter__(self):\n","        return self\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        self.stop_now = True"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T01:50:29.096763Z","iopub.status.busy":"2023-11-24T01:50:29.096092Z","iopub.status.idle":"2023-11-24T01:50:29.116303Z","shell.execute_reply":"2023-11-24T01:50:29.115174Z","shell.execute_reply.started":"2023-11-24T01:50:29.096729Z"},"trusted":true},"outputs":[],"source":["def generate(model, tokenizer, prompt, generation_config):\n","    data = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    generate_params = {\n","        **data,\n","        \"generation_config\": generation_config,\n","    }\n","\n","    # Stream the reply 1 token at a time.\n","    # This is based on the trick of using 'stopping_criteria' to create an iterator,\n","    # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n","\n","    all_tokens = list(range(len(tokenizer)))\n","    next_tokens = []\n","\n","    def prefix_allowed_tokens_fn(batch_id, input_ids):\n","        if len(next_tokens) > 0:\n","            allowed_tokens = [next_tokens.pop(0)]\n","            # print(allowed_tokens)\n","            return allowed_tokens\n","        return all_tokens\n","\n","    def generate_with_callback(callback=None, **kwargs):\n","        if \"stopping_criteria\" in kwargs:\n","            del kwargs[\"stopping_criteria\"]\n","        kwargs.setdefault(\n","            \"stopping_criteria\", StoppingCriteriaList()\n","        )\n","        kwargs[\"stopping_criteria\"].append(\n","            Stream(callback_func=callback)\n","        )\n","        with torch.no_grad():\n","            model.generate(\n","                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n","                **kwargs\n","            )\n","\n","    def generate_with_streaming(**kwargs):\n","        return Iteratorize(\n","            generate_with_callback, kwargs, callback=None\n","        )\n","\n","    def get_calc_result(expression):\n","        try:\n","            expression = re.sub(r'[^0-9+\\-*/.()]', '', expression.replace(',', '.'))\n","            result = str(round(eval(expression), 2))\n","            if '.' in result:\n","                left, right = result.split('.', 1)\n","                if set(right) == {'0'}:\n","                    result = left\n","            # print(expression, '=', result)\n","            return result\n","        except Exception as ex:\n","            print(expression, ex)\n","            return None\n","\n","    is_end = False\n","    last_calculated_idx = 0\n","    # calc_start, calc_end = '<calculator>', '</calculator>'\n","    calc_start, calc_end = '[[', ']]'\n","\n","    for _ in range(100):\n","        if is_end:\n","            break\n","        with generate_with_streaming(**generate_params) as generator:\n","            for output in generator:\n","                if output[-1] == tokenizer.eos_token_id or \\\n","                        len(output) == generate_params['generation_config'].max_new_tokens - 1:\n","                    is_end = True\n","                    yield decoded_output\n","                    break\n","                decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n","                new_output = decoded_output[last_calculated_idx:]\n","                # print('new_output:', last_calculated_idx, new_output)\n","                if calc_end in new_output:\n","                    exression = new_output.rsplit(calc_end, 1)[0]\n","                    if calc_start in exression:\n","                        exression = exression.rsplit(calc_start, 1)[1].strip()\n","                        result = get_calc_result(exression)\n","                        if result is not None:\n","                            result_tokens = tokenizer(result, add_special_tokens=False)['input_ids']\n","                            # print(result, result_tokens)\n","                            next_tokens.extend(result_tokens)\n","                            yield decoded_output\n","                    last_calculated_idx = len(decoded_output)\n","\n","\n","def simple_generate(model, tokenizer, prompt, generation_config):\n","    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n","    with torch.no_grad():\n","        output_ids = model.generate(\n","            **data,\n","            generation_config=generation_config\n","        )[0]\n","    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n","    return output.strip()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["400"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def get_answer(a):\n","    numbers = re.findall(r'(\\d+\\.\\d+|\\d+\\,\\d+|\\d+)', a)\n","    if len(numbers) == 0:\n","        return float('inf')\n","    return float(numbers[-1].replace(',', '.'))\n","\n","gsm_test = pd.read_csv('gsm_test.csv')\n","\n","actual_answers = [\n","    float(a.rsplit('#### ', 1)[1].strip())\n","    for a in gsm_test['answer']\n","]\n","len(actual_answers)"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuned evaluation"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def make_prompt(input):\n","    # prompt = f'<SC6>Задача: {input}\\nРешение: <extra_id_0>\\nОтвет: <extra_id_1></s>'\n","    # prompt = f'<SC6>Задача: {input}\\nРешение: <extra_id_0>####<extra_id_1>'\n","    prompt = f'<CS6>Задача: {input}\\nРешение: <extra_id_0>\\nОтвет: <extra_id_1>'\n","    return prompt"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<CS6>Задача: Если Анне 9 лет, а ее брат вдвое старше ее, сколько лет будет ее брату через 3 года?\n","Решение: <extra_id_0>\n","Ответ: <extra_id_1>\n","<extra_id_0> 9 * 2 = [[9 * 2]]18 лет.\n","3 = [[3]]3 года. <extra_id_1> 3\n","\n","==============================\n","<CS6>Задача: Доставка пиццы Эшли стоит 15 долларов. Какова общая сумма, которую Эшли должна дать доставщику, если она хочет дать чаевые, равные 1/5 заказанной суммы?\n","Решение: <extra_id_0>\n","Ответ: <extra_id_1>\n","<extra_id_0> Чаевые составляют 15 / 5 = [[15 / 5]]3.\n","Таким образом, общая сумма составляет 15 + 3 = [[15 + 3]]18. <extra_id_1> 18\n","\n","==============================\n"]}],"source":["inputs = [\n","    \"Если Анне 9 лет, а ее брат вдвое старше ее, сколько лет будет ее брату через 3 года?\",\n","    \"Доставка пиццы Эшли стоит 15 долларов. Какова общая сумма, которую Эшли должна дать доставщику, если она хочет дать чаевые, равные 1/5 заказанной суммы?\"\n","]\n","for inp in inputs:\n","    prompt = make_prompt(inp)\n","    print(prompt)\n","    result = ''\n","    for output in generate(model, tokenizer, prompt, generation_config):\n","        # print(f'###{output}###')\n","        result = output\n","    print(result)\n","    print(\"\\n==============================\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import os\n","gen_dir = f'./generations/{MODEL_CHKP}'\n","os.makedirs(gen_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["400it [20:50,  3.13s/it]\n"]}],"source":["generated_outputs = []\n","with open(f'{gen_dir}/gen_t5_ft_output.txt', 'a') as f_out:\n","    for i, d in tqdm(enumerate(gsm_test.to_dict('records'))):\n","        if i < len(generated_outputs):\n","            continue\n","        prompt = make_prompt(d['question'])\n","        result = ''\n","        for output in generate(model, tokenizer, prompt, generation_config):\n","            result = output\n","        generated_outputs.append(result)\n","        f_out.write(f'\\n### OUTPUT {i} ###\\n{output}\\n')\n","        f_out.flush()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# with open(f'{gen_dir}/gen_t5_ft_output.txt', 'r') as f_in:\n","#     generated_outputs = re.split(r'\\n### OUTPUT \\d+ ###\\n', f_in.read())[1:]"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["400"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["len(generated_outputs)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["with open(f'{gen_dir}/gen_t5_ft_answers.pkl', 'wb') as f:\n","    pickle.dump(generated_outputs, f)\n","\n","# with open(f'{gen_dir}/gen_t5_ft_answers.pkl', 'rb') as f:\n","#     generated_outputs = pickle.load(f)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["0.175"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["generated_answers = [get_answer(a) for a in generated_outputs]\n","np.isclose(generated_answers, actual_answers, rtol=1e-9, atol=1e-3).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuned evaluation (no calculator)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# def make_prompt_lm(input):\n","#     prompt = f'<LM>Задача: {input}\\nРешение: '\n","#     return prompt\n","\n","def make_prompt(input):\n","    prompt = f'<CS6>Задача: {input}\\nРешение: <extra_id_0>\\nОтвет: <extra_id_1>'\n","    return prompt"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<extra_id_0> \n","Брату Анны 9 лет * 2 = 15 лет.\n","Таким образом, через 3 года ему будет 15 лет. <extra_id_1> \n","15 лет\n","\n","==============================\n","<extra_id_0> \n","Эшли должна заплатить 15 долларов + 15 долларов = 20 долларов.\n","Таким образом, она должна дать чаевые в размере 1 / 5 от 20 долларов = 15 долларов.\n","Таким образом, она должна дать чаевые в размере 15 долларов - 15 долларов = 5 долларов. <extra_id_1> \n","5 долларов\n","\n","==============================\n"]}],"source":["inputs = [\n","    \"Если Анне 9 лет, а ее брат вдвое старше ее, сколько лет будет ее брату через 3 года?\",\n","    \"Доставка пиццы Эшли стоит 15 долларов. Какова общая сумма, которую Эшли должна дать доставщику, если она хочет дать чаевые, равные 1/5 заказанной суммы?\"\n","]\n","for inp in inputs:\n","    prompt = make_prompt(inp)\n","    # print(prompt)\n","    output = simple_generate(model, tokenizer, prompt, generation_config)\n","    print(output)\n","    print(\"\\n==============================\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os\n","gen_dir = f'./generations/{MODEL_CHKP}'\n","os.makedirs(gen_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["400it [18:44,  2.81s/it]\n"]}],"source":["generated_outputs_no_calc = []\n","with open(f'{gen_dir}/gen_t5_ft_output_no_calc.txt', 'a') as f_out:\n","    for i, d in tqdm(enumerate(gsm_test.to_dict('records'))):\n","        if i < len(generated_outputs_no_calc):\n","            continue\n","        prompt = make_prompt(d['question'])\n","        output = simple_generate(model, tokenizer, prompt, generation_config)\n","        generated_outputs_no_calc.append(output)\n","        f_out.write(f'\\n### OUTPUT {i} ###\\n{output}\\n')\n","        f_out.flush()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["400"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["# with open(f'{gen_dir}/gen_t5_ft_output_no_calc.txt', 'r') as f_in:\n","#     generated_outputs_no_calc = re.split(r'\\n### OUTPUT \\d+ ###\\n', f_in.read())[1:]\n","\n","len(generated_outputs_no_calc)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["with open(f'{gen_dir}/gen_t5_ft_answers_no_calc.pkl', 'wb') as f:\n","    pickle.dump(generated_outputs_no_calc, f)\n","\n","# with open(f'{gen_dir}/gen_t5_ft_answers_no_calc.pkl', 'rb') as f:\n","#     generated_outputs_no_calc = pickle.load(f)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["0.0325"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["generated_answers = [get_answer(a) for a in generated_outputs_no_calc]\n","np.isclose(generated_answers, actual_answers, rtol=1e-9, atol=1e-3).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for g, a in zip(generated_answers, actual_answers):\n","    if g == a:\n","        print(g, a)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for g, a in zip(generated_answers, actual_answers):\n","    if g != a:\n","        print(g, a)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4047770,"sourceId":7035981,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
